{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the url collection\n",
    "\n",
    "After being able to fully scrape the webpage for it's statistics. I wanted to automate the process of finding URLs. The vision is that this URL fetcher will store the URLs in some sort of file.\n",
    "\n",
    "Then the webscraper goes through each URL in that file to extract the data, and that's about it.\n",
    "\n",
    "List of URLs --> webscraper(repeated over every URL) --> CSV\n",
    "\n",
    "My simple three step program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# As of the time of writing, the input is a local HTML file\n",
    "import urllib.request\n",
    "\n",
    "url = \"\"\n",
    "\n",
    "fp = urllib.request.urlopen(url)\n",
    "mybytes = fp.read()\n",
    "\n",
    "webpage = mybytes.decode(\"utf8\")\n",
    "fp.close()\n",
    "\n",
    "link_parent = re.findall(r'(<div class=\"scroll-wrap scroll-wrap-boxscore\" id=\"main__schedule__match\">[\\s\\S]*?<div class=\"home-league__content__entry home-league__content__entry__leaders\">)', webpage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about three thousand lines in link_parent alone. That's okay though because we know the exact pattern of the URLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = re.findall(r'href=\"(/basketball/game/[\\s\\S]*?)\"', link_parent[0])\n",
    "print(len(links)) # 2274 games as of 2024-08-02\n",
    "# Range of games are from 2024-10-25 to 2024-08-02\n",
    "\n",
    "# The links are relative, so we need to add the domain to them\n",
    "links = list(map(lambda x: \"\" + x, links))\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that's left is to store these links in a file of our chosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "As of yet, I have no real way to find all of the links, without selecting all the days to load the full HTML. I would have to further analyze the JavaScript the website uses in order to make a proper data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "Upon runnning the webscraping script on the first 100 links. That way I can handle errors as well as not accidentally DDoS proballers.\n",
    "\n",
    "I realized that my links file has duplicate links. I figured this out be the resulting datasets were only 33 instead of 100.\n",
    "So let's fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "links = set(links)\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links.txt', 'w') as f:\n",
    "    for line in links:\n",
    "        f.write(\"%s\\n\" % line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
