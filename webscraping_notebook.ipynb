{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "Okie dokie so by using some regex patterns in Python I was able to find both tables, they are stored inside of a parent div with class name \"table__inner\". From there we can extract information from both tables.\n",
    "\n",
    "Table headers are neatly stored inside of the \\<th> tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "webpage = open(\"./Denver Nuggets vs. Los Angeles Lakers - Oct 25, 2023 - Game recap Proballers.htm\")\n",
    "tables =  re.findall(r'(<div class=\\\"table__inner\\\">[\\s\\S]*?<\\/table>)', webpage.read())\n",
    "\n",
    "team_1 = tables[0]\n",
    "team_2 = tables[1]\n",
    "\n",
    "team_headers = re.findall(r'(<th class=\\\".*\\\">.*<\\/th>)', team_1)\n",
    "team_headers = [re.sub(r'(<th class=\\\".*\\\">|<\\/th>)', '', header) for header in team_headers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both tables have the exact same column names, the headers only need to be extracted once.\n",
    "\n",
    "Now we can get to extracting all the data. Which is stored inside of the \\<td> tags. Some data such as player names are nested in an anchor tag which makes them slightly more challenging to extract.\n",
    "\n",
    "However with another regex pattern we can remove that anchor tag, but it's left with a lot of whitespaces. So the best I can do is make all the names camel case. Which is thankfully still readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA FROM TEAM 1 TABLE\n",
    "team_1_data_raw = re.findall(r'(<td class=\\\".*\\\">[\\s\\S]*?<\\/td>)', team_1)\n",
    "team_1_data_raw = [re.sub(r'(<td class=\\\".*\\\">|<\\/td>|<a [\\s\\S]*?>\\n|\\n|<\\/a>|<div [\\s\\S]*?>[\\s\\S]*?>[\\s\\S]*?<\\/div>)','', data) for data in team_1_data_raw]\n",
    "\n",
    "# GET DATA FROM TEAM 2 TABLE\n",
    "team_2_data_raw = re.findall(r'(<td class=\\\".*\\\">[\\s\\S]*?<\\/td>)', team_2)\n",
    "team_2_data_raw = [re.sub(r'(<td class=\\\".*\\\">|<\\/td>|<a [\\s\\S]*?>\\n|\\n|<\\/a>|<div [\\s\\S]*?>[\\s\\S]*?>[\\s\\S]*?<\\/div>)','', data) for data in team_2_data_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE OBJECT TO INSERT INTO DATAFRAME\n",
    "team_1_data = {team_headers[i]: [] for i in range(len(team_headers))}\n",
    "team_2_data = {team_headers[i]: [] for i in range(len(team_headers))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATE THROUGH TEAM 1 AND TEAM 2 DATA AND INSERT INTO OBJECT\n",
    "row_iterator = 0\n",
    "for i in range(len(team_1_data_raw)):\n",
    "    if row_iterator == len(team_headers):\n",
    "        row_iterator = 0\n",
    "        team_1_data[team_headers[row_iterator]].append(team_1_data_raw[i])\n",
    "        team_2_data[team_headers[row_iterator]].append(team_2_data_raw[i])\n",
    "        row_iterator += 1\n",
    "    else:\n",
    "        team_1_data[team_headers[row_iterator]].append(team_1_data_raw[i])\n",
    "        team_2_data[team_headers[row_iterator]].append(team_2_data_raw[i])\n",
    "        row_iterator += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove whitespace from player names\n",
    "team_1_data['PLAYER'] = [re.sub(r'(\\s)', '', player) for player in team_1_data['PLAYER']]\n",
    "team_2_data['PLAYER'] = [re.sub(r'(\\s)', '', player) for player in team_2_data['PLAYER']]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate columns\n",
    "\n",
    "The website I am currently scraping displays three columns twice for readability.\n",
    "\n",
    "- Pts\n",
    "- Ast\n",
    "- Reb\n",
    "\n",
    "This is a problem since now these three lists are twice the size as every other list.\n",
    "\n",
    "We also know that all the other columns are of length 13, and these three are of length 26. Therefore we should be able to split these three columns in half very quickly and go along our merry way to put them into a pandas dataframe.\n",
    "\n",
    "It will be noteably easier to cut these lists in half since our raw data is taken in order of from left to right. So messing with my iteration doesn't seem wise. Especially when Python has these features to quickly remove elements like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the length of Pts, Reb and Ast lists in half\n",
    "# Pts\n",
    "team_1_data['Pts'] = team_1_data['Pts'][0:len(team_1_data['Pts'])//2]\n",
    "team_2_data['Pts'] = team_2_data['Pts'][0:len(team_2_data['Pts'])//2]\n",
    "# Reb\n",
    "team_1_data['Reb'] = team_1_data['Reb'][0:len(team_1_data['Reb'])//2]\n",
    "team_2_data['Reb'] = team_2_data['Reb'][0:len(team_2_data['Reb'])//2]\n",
    "# Ast\n",
    "team_1_data['Ast'] = team_1_data['Ast'][0:len(team_1_data['Ast'])//2]\n",
    "team_2_data['Ast'] = team_2_data['Ast'][0:len(team_2_data['Ast'])//2]\n",
    "\n",
    "# Remove duplicate keys from team_1_data and team_2_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally...\n",
    "\n",
    "We should be all set to insert our values into a pandas dataframe\n",
    "\n",
    "However, with duplicate values, comes duplicate columns. As you can see from the following print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADER:  PLAYER   13\n",
      "\n",
      "\n",
      "HEADER:  Pts   13\n",
      "\n",
      "\n",
      "HEADER:  Reb   13\n",
      "\n",
      "\n",
      "HEADER:  Ast   13\n",
      "\n",
      "\n",
      "HEADER:  MIN   13\n",
      "\n",
      "\n",
      "HEADER:  2M-2A   13\n",
      "\n",
      "\n",
      "HEADER:  3M-3A   13\n",
      "\n",
      "\n",
      "HEADER:  FG%   13\n",
      "\n",
      "\n",
      "HEADER:  1M-1A   13\n",
      "\n",
      "\n",
      "HEADER:  1%   13\n",
      "\n",
      "\n",
      "HEADER:  Or   13\n",
      "\n",
      "\n",
      "HEADER:  Dr   13\n",
      "\n",
      "\n",
      "HEADER:  Reb   13\n",
      "\n",
      "\n",
      "HEADER:  Ast   13\n",
      "\n",
      "\n",
      "HEADER:  To   13\n",
      "\n",
      "\n",
      "HEADER:  Stl   13\n",
      "\n",
      "\n",
      "HEADER:  Blk   13\n",
      "\n",
      "\n",
      "HEADER:  Fo   13\n",
      "\n",
      "\n",
      "HEADER:  Pts   13\n",
      "\n",
      "\n",
      "HEADER:  +/-   13\n",
      "\n",
      "\n",
      "HEADER:  Eff   13\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# team_1_panda = pd.DataFrame(team_1_data)\n",
    "# team_2_panda = pd.DataFrame(team_2_data)\n",
    "\n",
    "for i in range(len(team_headers)):\n",
    "    print(\"HEADER: \",team_headers[i],\" \",len(team_1_data[team_headers[i]]))\n",
    "    print(\"\\n\")\n",
    "    # print(\"HEADER: \",team_headers[i],\" \",len(team_2_data[team_headers[i]]))\n",
    "    # print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pandas\n",
    "\n",
    "Duplicate columns aren't scary though. Since there are easy ways built into to pandas to determine if a column is duplicate and to remove them.\n",
    "\n",
    "Thank you pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "# Insert into dataframe\n",
    "team_1_panda = pd.DataFrame(team_1_data)\n",
    "team_2_panda = pd.DataFrame(team_2_data)\n",
    "\n",
    "# Remove duplicate keys from team_1_panda and team_2_panda\n",
    "team_1_panda = team_1_panda.loc[:,~team_1_panda.columns.duplicated()]\n",
    "team_2_panda = team_2_panda.loc[:,~team_2_panda.columns.duplicated()]\n",
    "print(team_1_panda)\n",
    "print(team_2_panda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_1_panda['Pts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "Now with our cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "team_1_panda.to_csv('team_1.csv', index=False)\n",
    "team_2_panda.to_csv('team_2.csv' ,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
